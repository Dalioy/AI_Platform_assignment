build a computational graph that follows this general structure:

input (x)
   │
   ├──> Layer 1 (3 neurons: w00,b00 / w01,b01 / w02,b02)
   │        ↓ Activation: ReLU
   │
   ├──> Layer 2 (2 neurons: w10,b10 / w11,b11)
   │        ↓ Activation: Sigmoid
   │
   ├──> Combine the two outputs (+)
   │        ↓ Activation: Tanh
   │
   └──> Output layer (1 neuron: w20,b20)
            ↓ Activation: None (linear output)

Build the entire computational graph manually using PyTorch tensors. Create all weights and biases as tensors with requires_grad=True. Apply mathematical operations manually (for example, y = torch.relu(w * x + b)). Your graph should include at least three activation functions: ReLU, Sigmoid, and Tanh. Perform a complete forward pass step by step, showing intermediate results and printing them. After computing the final output, use output.backward() to calculate the gradient of the output with respect to the input and print it. 
============
Deadline & Submission Instructions:
The assignment must be submitted by Thursday, October 9th, at 11:59 PM.
For submission, please upload your code to GitHub and then submit your repository URL using the following form:
https://forms.gle/mnJLtHyiRNGqfTp59